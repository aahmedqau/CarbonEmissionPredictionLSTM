# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eYqW7Ebg3QAmbISmAOxEBCGHPSzkvWT0
"""



# ================================
# RankRAG + LSTM (Rank-LSTM)
# ================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import precision_score, recall_score, mean_squared_error
from sklearn.model_selection import train_test_split

# ==========================================
# 1. Load Dataset
# ==========================================

data = pd.read_csv("/content/Dataset.csv")

# Assume 'Value' column is CO2 emission target based on previous cell's context
target_column = 'Value'

# Handle missing values (apply only to numeric columns for ffill)
# First, convert 'Value' to numeric, as it may contain non-numeric entries like '<TRUNCATED>' from preview
data['Value'] = pd.to_numeric(data['Value'], errors='coerce')
# Fill missing values only for numeric columns
data = data.fillna(method="ffill")

# ==========================================
# 2. RankRAG Context Ranking (Feature Ranking)
# ==========================================

# Select only numeric columns for correlation calculation
numeric_data = data.select_dtypes(include=np.number)

# Rank features by correlation with target
# Ensure target_column exists in numeric_data
if target_column not in numeric_data.columns:
    raise ValueError(f"Target column '{target_column}' not found in numeric data.")

correlations = numeric_data.corr()[target_column].abs().sort_values(ascending=False)

# Select Top-k ranked features (excluding target)
top_k = 5
# Filter out the target column itself from selected features
selected_features = [col for col in correlations.index[1:top_k+1] if col != target_column]

print("Top Ranked Features:", selected_features)

# Filter dataset for selected features and target
X = data[selected_features].values
y = data[target_column].values.reshape(-1, 1)

# ==========================================
# 3. Normalize Data
# ==========================================

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(y)

# ==========================================
# 4. Create Time Series Sequences
# ==========================================

def create_sequences(X, y, seq_length=12):
    xs, ys = [], []
    for i in range(len(X) - seq_length):
        xs.append(X[i:i+seq_length])
        ys.append(y[i+seq_length])
    return np.array(xs), np.array(ys)

SEQ_LEN = 12
X_seq, y_seq = create_sequences(X, y, SEQ_LEN)

X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_seq, test_size=0.2, shuffle=False
)

# Convert to Torch tensors
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

# ==========================================
# 5. LSTM Model Definition
# ==========================================

class RankLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, dropout=0.2):
        super(RankLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        return out

# ==========================================
# 6. Training Function
# ==========================================

def train_model(lr=0.001, dropout=0.2, weight_decay=0.0, epochs=100):
    model = RankLSTM(input_size=X_train.shape[2], dropout=dropout)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X_train)
        loss = criterion(output, y_train)
        loss.backward()
        optimizer.step()

        train_losses.append(loss.item())

        model.eval()
        with torch.no_grad():
            val_output = model(X_test)
            val_loss = criterion(val_output, y_test)
            val_losses.append(val_loss.item())

    return model, train_losses, val_losses

# ==========================================
# 7. Precision vs Learning Rate
# ==========================================

learning_rates = [0.01, 0.018, 0.026, 0.034, 0.042, 0.05]
precision_scores = []

for lr in learning_rates:
    model, _, _ = train_model(lr=lr)
    model.eval()
    with torch.no_grad():
        preds = model(X_test).numpy()
    # Using mean as a threshold for binary classification for precision/recall
    preds_binary = (preds > preds.mean()).astype(int)
    y_binary = (y_test.numpy() > y_test.numpy().mean()).astype(int)
    # Check if there are any positive predictions or actuals before calculating precision
    if np.sum(preds_binary) > 0 and np.sum(y_binary) > 0:
        precision_scores.append(precision_score(y_binary, preds_binary, zero_division=0))
    else:
        precision_scores.append(0.0) # Append 0 if no positive samples or predictions

plt.figure()
plt.plot(learning_rates, precision_scores, marker='o')
plt.title("Precision vs Learning Rate")
plt.xlabel("Learning Rate")
plt.ylabel("Precision")
plt.show()

# ==========================================
# 8. Recall vs Dropout
# ==========================================

dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]
recall_scores = []

for dr in dropouts:
    model, _, _ = train_model(dropout=dr)
    model.eval()
    with torch.no_grad():
        preds = model(X_test).numpy()
    preds_binary = (preds > preds.mean()).astype(int)
    y_binary = (y_test.numpy() > y_test.numpy().mean()).astype(int)
    # Check if there are any positive predictions or actuals before calculating recall
    if np.sum(preds_binary) > 0 and np.sum(y_binary) > 0:
        recall_scores.append(recall_score(y_binary, preds_binary, zero_division=0))
    else:
        recall_scores.append(0.0) # Append 0 if no positive samples or predictions

plt.figure()
plt.plot(dropouts, recall_scores, marker='o')
plt.title("Recall vs Dropout Rate")
plt.xlabel("Dropout Rate")
plt.ylabel("Recall")
plt.show()

# ==========================================
# 9. Recall vs Weight Decay
# ==========================================

weight_decays = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]
recall_wd = []

for wd in weight_decays:
    model, _, _ = train_model(weight_decay=wd)
    model.eval()
    with torch.no_grad():
        preds = model(X_test).numpy()
    preds_binary = (preds > preds.mean()).astype(int)
    y_binary = (y_test.numpy() > y_test.numpy().mean()).astype(int)
    # Check if there are any positive predictions or actuals before calculating recall
    if np.sum(preds_binary) > 0 and np.sum(y_binary) > 0:
        recall_wd.append(recall_score(y_binary, preds_binary, zero_division=0))
    else:
        recall_wd.append(0.0) # Append 0 if no positive samples or predictions

plt.figure()
plt.plot(weight_decays, recall_wd, marker='o')
plt.title("Recall vs Weight Decay")
plt.xlabel("Weight Decay")
plt.ylabel("Recall")
plt.show()

# ==========================================
# 10. Final Model Prediction Plot
# ==========================================

model, train_losses, val_losses = train_model(lr=0.03, dropout=0.3)

model.eval()
with torch.no_grad():
    predictions = model(X_test).numpy()

# Inverse scaling
predictions = scaler_y.inverse_transform(predictions)
actual = scaler_y.inverse_transform(y_test.numpy())

plt.figure(figsize=(10,5))
plt.plot(actual, label="Actual")
plt.plot(predictions, label="Predicted")
plt.title("LSTM Prediction of CO2 Emissions")
plt.legend()
plt.show()

# ==========================================
# 11. Training & Validation Loss Plot
# ==========================================

plt.figure()
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title("Training and Validation Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.legend()
plt.show()

