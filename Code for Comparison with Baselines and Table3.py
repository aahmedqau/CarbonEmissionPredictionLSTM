# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-d7mZCZbE1ALvliV932flxXRsPpPonoi
"""



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler

# ===============================
# Reproducibility
# ===============================
torch.manual_seed(42)
np.random.seed(42)

# ===============================
# 1. Load Dataset
# ===============================

data = pd.read_csv("/content/Dataset.csv")

target_column = "Value"
data[target_column] = pd.to_numeric(data[target_column], errors="coerce")
data = data.fillna(method="ffill")

values = data[target_column].values.reshape(-1, 1)

scaler = MinMaxScaler()
values = scaler.fit_transform(values)

# ===============================
# 2. Sequence Creation
# ===============================

def create_sequences(data, seq_len=12):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

SEQ_LEN = 12
X, y = create_sequences(values, SEQ_LEN)

X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

# ===============================
# 3. Model Definitions
# ===============================

# ---- Baseline 1: Bidirectional LSTM ----
class BiLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 64, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(128, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

# ---- Baseline 2: CNN-LSTM ----
class CNNLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3)
        self.lstm = nn.LSTM(32, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = torch.relu(self.conv1(x))
        x = x.permute(0, 2, 1)
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])

# ---- Proposed: RankRAG-LSTM ----
class RankRAGLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 64, num_layers=2, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(64, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.dropout(out[:, -1, :])
        return self.fc(out)

# ===============================
# 4. Training Function
# ===============================

def train_model(model, epochs=100):
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    losses = []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    return losses

# ===============================
# 5. Train All Models
# ===============================

epochs = 100

bilstm_losses = train_model(BiLSTM(), epochs)
cnnlstm_losses = train_model(CNNLSTM(), epochs)
rankrag_losses = train_model(RankRAGLSTM(), epochs)

# ===============================
# 6. Plot Comparison (MATCHES IMAGE)
# ===============================

plt.figure(figsize=(10,6))

plt.plot(bilstm_losses, label="Bidirectional LSTM", color="brown", linewidth=2)
plt.plot(cnnlstm_losses, label="CNN-LSTM", color="orange", linewidth=2)
plt.plot(rankrag_losses, label="RankRAG-LSTM", color="green", linewidth=2)

plt.xlabel("Epoch", fontsize=12)
plt.ylabel("MSE", fontsize=12)
plt.title("Training Loss Comparison of Models", fontsize=14)

plt.grid(True, linestyle="--", alpha=0.6)
plt.legend()
plt.tight_layout()
plt.show()



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout
import warnings
warnings.filterwarnings('ignore')

# Load and preprocess the dataset
def load_and_preprocess_data():
    # Parse the dataset
    data = []
    with open('/content/Dataset.csv', 'r') as f:
        next(f)  # Skip the header row
        for line in f:
            if not line.strip():
                continue
            parts = line.strip().split(',')
            if len(parts) >= 6:
                code = parts[0]
                date = parts[1]
                value = float(parts[2]) if parts[2] != 'Not Available' else np.nan
                category_code = int(parts[3])
                description = parts[4]
                units = parts[5]

                # Extract year and month
                year = int(date[:4])
                month = int(date[4:6])
                day = 1 # Add a default day

                data.append({
                    'code': code,
                    'year': year,
                    'month': month,
                    'day': day, # Include day in the dictionary
                    'value': value,
                    'category_code': category_code,
                    'description': description,
                    'units': units
                })

    # Convert to DataFrame
    df = pd.DataFrame(data)

    # Filter out NaN values
    df = df.dropna(subset=['value'])

    # Create a time series for each energy source
    energy_sources = {
        'Petroleum': 'PAEIEUS',
        'Residual Fuel Oil': 'RFEIEUS',
        'Natural Gas': 'NNEIEUS'
    }

    # Create time series for each energy source
    time_series = {}
    for source, code in energy_sources.items():
        source_data = df[df['code'] == code].copy()
        # Create a date string in 'YYYY-MM-DD' format before converting to datetime
        source_data['date'] = pd.to_datetime(
            source_data['year'].astype(str) + '-' +
            source_data['month'].astype(str).str.zfill(2) + '-' +
            source_data['day'].astype(str).str.zfill(2),
            errors='coerce' # Convert invalid dates to NaT
        )
        source_data = source_data.dropna(subset=['date']) # Remove rows with NaT dates
        source_data = source_data.sort_values('date')
        time_series[source] = source_data

    return time_series

# Prepare data for time series forecasting
def prepare_time_series(data, seq_length=12):
    # Create sequences for time series forecasting
    X, y = [], []

    # Sort by date
    data = data.sort_values('date')

    # Convert to numpy array
    values = data['value'].values

    # Create sequences
    for i in range(len(values) - seq_length):
        X.append(values[i:i+seq_length])
        y.append(values[i+seq_length])

    X = np.array(X)
    y = np.array(y)

    # Reshape for RNN input (samples, time steps, features)
    X = X.reshape((X.shape[0], X.shape[1], 1))

    return X, y

# Build and evaluate different RNN models
def evaluate_models(time_series, seq_length=12):
    results = []

    # For each energy source, create and evaluate models
    for source, data in time_series.items():
        # Prepare data
        X, y = prepare_time_series(data, seq_length)

        # Split into train and test (80% train, 20% test)
        split_idx = int(0.8 * len(X))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # Scale data
        scaler = MinMaxScaler(feature_range=(0, 1))
        X_train_scaled = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)
        X_test_scaled = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)
        y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1)).flatten()
        y_test_scaled = scaler.transform(y_test.reshape(-1, 1)).flatten()

        # 1. RNN Model
        rnn_model = Sequential([
            SimpleRNN(64, return_sequences=True, input_shape=(seq_length, 1)),
            SimpleRNN(32),
            Dense(1)
        ])
        rnn_model.compile(optimizer='adam', loss='mse')
        rnn_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, verbose=0)
        rnn_pred = rnn_model.predict(X_test_scaled)
        rnn_pred = scaler.inverse_transform(rnn_pred)
        rnn_mae = mean_absolute_error(y_test, rnn_pred)
        rnn_rmse = np.sqrt(mean_squared_error(y_test, rnn_pred))
        rnn_mse = mean_squared_error(y_test, rnn_pred)
        results.append({
            'model': 'Recurrent Neural Networks (RNN)',
            'mae': rnn_mae,
            'rmse': rnn_rmse,
            'mse': rnn_mse
        })

        # 2. LSTM Model
        lstm_model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),
            LSTM(32),
            Dense(1)
        ])
        lstm_model.compile(optimizer='adam', loss='mse')
        lstm_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, verbose=0)
        lstm_pred = lstm_model.predict(X_test_scaled)
        lstm_pred = scaler.inverse_transform(lstm_pred)
        lstm_mae = mean_absolute_error(y_test, lstm_pred)
        lstm_rmse = np.sqrt(mean_squared_error(y_test, lstm_pred))
        lstm_mse = mean_squared_error(y_test, lstm_pred)
        results.append({
            'model': 'Long Short-Term Memory (LSTM)',
            'mae': lstm_mae,
            'rmse': lstm_rmse,
            'mse': lstm_mse
        })

        # 3. Dual Path RNN (simplified version)
        dprnn_model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(seq_length, 1)),
            LSTM(32, return_sequences=True),
            LSTM(32),
            Dense(1)
        ])
        dprnn_model.compile(optimizer='adam', loss='mse')
        dprnn_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, verbose=0)
        dprnn_pred = dprnn_model.predict(X_test_scaled)
        dprnn_pred = scaler.inverse_transform(dprnn_pred)
        dprnn_mae = mean_absolute_error(y_test, dprnn_pred)
        dprnn_rmse = np.sqrt(mean_squared_error(y_test, dprnn_pred))
        dprnn_mse = mean_squared_error(y_test, dprnn_pred)
        results.append({
            'model': 'Dual Path Recurrent Neural Networks (DPRNNs)',
            'mae': dprnn_mae,
            'rmse': dprnn_rmse,
            'mse': dprnn_mse
        })

        # 4. GRU Model
        gru_model = Sequential([
            GRU(64, return_sequences=True, input_shape=(seq_length, 1)),
            GRU(32),
            Dense(1)
        ])
        gru_model.compile(optimizer='adam', loss='mse')
        gru_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, verbose=0)
        gru_pred = gru_model.predict(X_test_scaled)
        gru_pred = scaler.inverse_transform(gru_pred)
        gru_mae = mean_absolute_error(y_test, gru_pred)
        gru_rmse = np.sqrt(mean_squared_error(y_test, gru_pred))
        gru_mse = mean_squared_error(y_test, gru_pred)
        results.append({
            'model': 'Gated Recurrent Unit (GRUs)',
            'mae': gru_mae,
            'rmse': gru_rmse,
            'mse': gru_mse
        })

        # 5. RankRAG-LSTM (custom model - simplified version)
        rankrag_lstm_model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(seq_length, 1)),
            Dropout(0.2),
            LSTM(64),
            Dense(1)
        ])
        rankrag_lstm_model.compile(optimizer='adam', loss='mse')
        rankrag_lstm_model.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, verbose=0)
        rankrag_lstm_pred = rankrag_lstm_model.predict(X_test_scaled)
        rankrag_lstm_pred = scaler.inverse_transform(rankrag_lstm_pred)
        rankrag_lstm_mae = mean_absolute_error(y_test, rankrag_lstm_pred)
        rankrag_lstm_rmse = np.sqrt(mean_squared_error(y_test, rankrag_lstm_pred))
        rankrag_lstm_mse = mean_squared_error(y_test, rankrag_lstm_pred)
        results.append({
            'model': 'RankRAG-LSTM',
            'mae': rankrag_lstm_mae,
            'rmse': rankrag_lstm_rmse,
            'mse': rankrag_lstm_mse
        })

        # Break after first energy source to match the table structure
        break

    return results

# Format and print the results table
def print_results_table(results):
    # Print header
    print("Models | MAE | RMSE | MSE")
    print("-" * 55)

    # Print each row with proper formatting
    for res in results:
        model_name = res['model']
        # Add reference numbers as shown in the original table
        if "RNN" in model_name:
            model_name += " [48]"
        elif "LSTM" in model_name and "Dual" not in model_name:
            model_name += " [49]"
        elif "DPRNNs" in model_name:
            model_name += " [50]"
        elif "GRUs" in model_name:
            model_name += " [51]"

        # Format values to match the table
        mae = f"{res['mae']:.3f}"
        rmse = f"{res['rmse']:.3f}"
        mse = f"{res['mse']:.3f}"

        # Special formatting for the table as shown in the example
        print(f"{model_name} | {mae} | {rmse} | {mse}")

# Main execution
if __name__ == "__main__":
    # Load and preprocess the dataset
    time_series = load_and_preprocess_data()

    # Evaluate all models
    results = evaluate_models(time_series)

    # Print the results in the requested table format
    print_results_table(results)

